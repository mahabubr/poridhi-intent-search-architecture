{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score"
      ],
      "metadata": {
        "id": "AGldok4s32YF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "14nE9nts3FuC"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    GenerationConfig,\n",
        "    EarlyStoppingCallback,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "from tqdm import tqdm\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from functools import partial\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import random\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['WANDB_DISABLED']=\"true\""
      ],
      "metadata": {
        "id": "HgfBof9BkZnW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"microsoft/ms_marco\", \"v2.1\", split=\"train\")"
      ],
      "metadata": {
        "id": "KzKHY2xwkc9z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4ccb248-41e9-4d90-e48f-22b46c5903af"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.select(range(200))"
      ],
      "metadata": {
        "id": "modKzS0X8qSl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    return text.strip().lower()"
      ],
      "metadata": {
        "id": "bp62gO9h8il7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(lambda example: {\n",
        "    'query': clean_text(example['query']),\n",
        "    'answers': [clean_text(ans) for ans in example['answers']]\n",
        "}, remove_columns=dataset.column_names)"
      ],
      "metadata": {
        "id": "j3sXTc8Q8fh4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_query_difficulty(example):\n",
        "  query = example['query']\n",
        "\n",
        "  word_count = len(query.split())\n",
        "  punctuation_count = sum(1 for c in query if c in [',', '.', '?', '!', ':', ';'])\n",
        "\n",
        "  length_score = len(query)\n",
        "\n",
        "  difficulty = word_count + punctuation_count + (length_score / 50)\n",
        "\n",
        "  example['difficulty'] = difficulty\n",
        "\n",
        "  return example"
      ],
      "metadata": {
        "id": "Jq0J_vx4_bmc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(compute_query_difficulty)"
      ],
      "metadata": {
        "id": "57z5t-ns_sGR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.sort(\"difficulty\")"
      ],
      "metadata": {
        "id": "gqQwthHE875r"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset.select(range(80))"
      ],
      "metadata": {
        "id": "Xlub6V1flEHr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = dataset.select(range(80, 100))"
      ],
      "metadata": {
        "id": "4wGjOVsPq_sQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_dtype = getattr(torch, \"float16\")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type='nf4',\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_use_double_quant=False,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "8daiEVoklMbR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name='google/flan-t5-large'\n",
        "\n",
        "device_map = {\"\": 0}"
      ],
      "metadata": {
        "id": "WvuCMhahlYh6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True,\n",
        "  )"
      ],
      "metadata": {
        "id": "f4nVIABHliYV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    padding_side=\"left\",\n",
        "    add_eos_token=True,\n",
        "    add_bos_token=True,\n",
        "    use_fast=False\n",
        "  )\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "vXufmFJ5lq8j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f80066c-eef7-4164-b9e8-d86612629268"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gen(model, prompt, length):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(**inputs, max_length=length, do_sample=True, top_p=0.95, temperature=0.8)\n",
        "\n",
        "    return [tokenizer.decode(outputs[0], skip_special_tokens=True)]"
      ],
      "metadata": {
        "id": "nA7ujGTsqPp2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "index = 2\n",
        "\n",
        "prompt = dataset[index]['query']\n",
        "summary = dataset[index]['answers'][0]\n",
        "\n",
        "formatted_prompt = f\"Instruct: Refine this user search query.\\n{prompt}\"\n",
        "\n",
        "res = gen(original_model,formatted_prompt,100,)\n",
        "\n",
        "output = res[0]\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN ANSWER:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dB54XO0lyEo",
        "outputId": "a019cba8-7a9b-460a-88b6-ecd88b7d7a0a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "INPUT PROMPT:\n",
            "Instruct: Refine this user search query.\n",
            "define traumatic\n",
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN ANSWER:\n",
            "it is extremely upsetting or something that causes great harm or damage to the mind or body.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATION - ZERO SHOT:\n",
            "<pad> definition of traumatic\n",
            "CPU times: user 733 ms, sys: 200 ms, total: 933 ms\n",
            "Wall time: 1.82 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt_formats(sample):\n",
        "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
        "\n",
        "    prompt_templates = [\n",
        "      \"### Instruct: Refine this user search query:\",\n",
        "      \"### Task: Improve the clarity of the following search query:\",\n",
        "      \"### Instruction: Fix the grammar and phrasing of this e-commerce search input:\",\n",
        "      \"### Command: Clean up this product search term:\",\n",
        "      \"### Request: Make this user query more natural and readable:\",\n",
        "      \"### Action: Rephrase this customer search for better understanding:\",\n",
        "    ]\n",
        "\n",
        "    instruction = random.choice(prompt_templates) + f\"\\n{sample['query'].strip()}\"\n",
        "    target = sample['answers'][0].strip()\n",
        "\n",
        "    return {\n",
        "        \"input\": f\"{INTRO_BLURB}\\n\\n{instruction}\",\n",
        "        \"target\": f\"{target}\"\n",
        "    }"
      ],
      "metadata": {
        "id": "GLE210QVmiBO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_length(model):\n",
        "    conf = model.config\n",
        "    max_length = None\n",
        "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
        "        max_length = getattr(model.config, length_setting, None)\n",
        "        if max_length:\n",
        "            print(f\"Found max lenth: {max_length}\")\n",
        "            break\n",
        "    if not max_length:\n",
        "        max_length = 1024\n",
        "        print(f\"Using default max length: {max_length}\")\n",
        "    return max_length"
      ],
      "metadata": {
        "id": "IKCd0aCzm39F"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_batch(batch, tokenizer, max_length):\n",
        "    model_inputs = tokenizer(\n",
        "        batch[\"input\"],\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\",\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "    labels = tokenizer(\n",
        "        batch[\"target\"],\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "1E8d3NYVnLGl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, dataset):\n",
        "    \"\"\"Format & tokenize it so it is ready for training\n",
        "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
        "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Preprocessing dataset...\")\n",
        "    dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
        "\n",
        "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
        "\n",
        "    dataset = dataset.map(\n",
        "        _preprocessing_function,\n",
        "        batched=True,\n",
        "    )\n",
        "\n",
        "    dataset = dataset.remove_columns(['answers', 'query', 'difficulty', \"input\", \"target\"])\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "ZEumLAz8nO0T"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = get_max_length(original_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SGgrkp6ngWM",
        "outputId": "f8725042-cb8c-4a09-9501-91c189ba2e38"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found max lenth: 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = preprocess_dataset(tokenizer, max_length, train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFFZe_Iyr2nk",
        "outputId": "41e46057-8a1c-4f44-d85b-360f2fceba82"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = preprocess_dataset(tokenizer, max_length, eval_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rzyk1zVT9ag5",
        "outputId": "3d668492-1f2f-4920-c4cf-7ad189ba515b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_yGZXLoZMBt",
        "outputId": "7208fd80-0b93-4a6f-a616-181fbc7a646c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 80\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJxKArRAaLwg",
        "outputId": "e729811d-ee6a-47b0-8fbd-2d8335b1caf6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 20\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(train_dataset[0]['input_ids'], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n7lj-VhdypR",
        "outputId": "598d5d5f-bf9f-4916-df47-8488801fca5e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: Fix the grammar and phrasing of this e-commerce search input: depona ab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_model = prepare_model_for_kbit_training(original_model)"
      ],
      "metadata": {
        "id": "VrZ3SgVbnuld"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"SEQ_2_SEQ_LM\",\n",
        ")"
      ],
      "metadata": {
        "id": "Ha3YJBr2nzhZ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_model.gradient_checkpointing_enable()"
      ],
      "metadata": {
        "id": "kgsWJG-wn0-g"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = get_peft_model(original_model, config)"
      ],
      "metadata": {
        "id": "-2rCe-fcn2t9"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_params = 0\n",
        "    all_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(f\"Trainable params: {trainable_params}\")\n",
        "    print(f\"All params: {all_params}\")\n",
        "    print(f\"Trainable%: {100 * trainable_params / all_params:.2f}%\")\n"
      ],
      "metadata": {
        "id": "CUSn2jeKs6Nt"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_number_of_trainable_model_parameters(peft_model)"
      ],
      "metadata": {
        "id": "5qcVP6KWn4XB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "957e2bf7-af2b-4db2-8296-6fe342df37e6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable params: 9437184\n",
            "All params: 503180288\n",
            "Trainable%: 1.88%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = f'./peft-flan-t5-training-{str(int(time.time()))}'"
      ],
      "metadata": {
        "id": "KlpDnd6on7zr"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model.config.use_cache = False"
      ],
      "metadata": {
        "id": "Xmgc-vPAoBFI"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_training_args = TrainingArguments(\n",
        "    output_dir = output_dir,\n",
        "    warmup_steps=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=2,\n",
        "    max_steps=5,\n",
        "    learning_rate=2e-4,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    logging_steps=5,\n",
        "    logging_dir=\"./logs\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=5,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=5,\n",
        "    do_eval=True,\n",
        "    gradient_checkpointing=True,\n",
        "    report_to=\"none\",\n",
        "    overwrite_output_dir = 'True',\n",
        "    group_by_length=True,\n",
        "    load_best_model_at_end=True,\n",
        "    dataloader_pin_memory=True,\n",
        "    dataloader_num_workers=2,\n",
        "    fp16=True\n",
        ")"
      ],
      "metadata": {
        "id": "pW53TY0NtL2e"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStoppingCallback(\n",
        "    early_stopping_patience=5,\n",
        "    early_stopping_threshold=0.0\n",
        "    )"
      ],
      "metadata": {
        "id": "vM2D8tOOB9Mi"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator=DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=peft_model,\n",
        "    padding=\"longest\",\n",
        "    pad_to_multiple_of=8\n",
        ")"
      ],
      "metadata": {
        "id": "-K2w5c2jJjQS"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample processed batch:\")\n",
        "sample_batch = data_collator([train_dataset[0], train_dataset[1]])\n",
        "print(\"Input shapes:\", sample_batch[\"input_ids\"].shape)\n",
        "print(\"Label shapes:\", sample_batch[\"labels\"].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0ouUtRDRME1",
        "outputId": "38fe1772-1bc0-434f-a60d-a3390dda9dfd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample processed batch:\n",
            "Input shapes: torch.Size([2, 512])\n",
            "Label shapes: torch.Size([2, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")"
      ],
      "metadata": {
        "id": "IiyRR7AgULUS"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "\n",
        "    print(type(preds), type(preds[0]))\n",
        "    print(preds[0])\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    bleu_result = bleu.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n",
        "\n",
        "    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    return {\n",
        "        \"bleu\": bleu_result[\"bleu\"],\n",
        "        \"rouge1\": rouge_result[\"rouge1\"],\n",
        "        \"rouge2\": rouge_result[\"rouge2\"],\n",
        "        \"rougeL\": rouge_result[\"rougeL\"]\n",
        "    }\n"
      ],
      "metadata": {
        "id": "d_sI3ZXIUPFh"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_trainer = transformers.Trainer(\n",
        "    model=peft_model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    args=peft_training_args,\n",
        "    data_collator=data_collator,\n",
        "    # callbacks=[early_stopping],\n",
        "    # compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "40VFaPIeoCbo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83d40981-c710-4cce-e7dc-d076fef1ed72"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peft_trainer.train()"
      ],
      "metadata": {
        "id": "Z4ZjqAAUoEO5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "aa0d7816-ff34-4e2c-c37f-1fa2a98b28d5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 00:43, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5, training_loss=0.0, metrics={'train_runtime': 54.0214, 'train_samples_per_second': 1.481, 'train_steps_per_second': 0.093, 'total_flos': 186700718407680.0, 'train_loss': 0.0, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Eb5kJfEa7XC"
      },
      "execution_count": 44,
      "outputs": []
    }
  ]
}