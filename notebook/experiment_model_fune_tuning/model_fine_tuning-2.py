# -*- coding: utf-8 -*-
"""model_fine_tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qCuxV6eQeNFVO-nj2x0ISBdvW9ukU0af
"""

!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score

from datasets import load_dataset
import transformers
from transformers import (
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    HfArgumentParser,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    GenerationConfig,
    EarlyStoppingCallback,
    DataCollatorForSeq2Seq
)
from tqdm import tqdm
from trl import SFTTrainer
import torch
import time
import pandas as pd
import numpy as np
import os
from functools import partial
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import random
import evaluate

os.environ['WANDB_DISABLED']="true"

dataset = load_dataset("microsoft/ms_marco", "v2.1", split="train")

dataset = dataset.select(range(120))

def clean_text(text):
    return text.strip().lower()

dataset = dataset.map(lambda example: {
    'query': clean_text(example['query']),
    'answers': [clean_text(ans) for ans in example['answers']]
}, remove_columns=dataset.column_names)

def compute_query_difficulty(example):
  query = example['query']

  word_count = len(query.split())
  punctuation_count = sum(1 for c in query if c in [',', '.', '?', '!', ':', ';'])

  length_score = len(query)

  difficulty = word_count + punctuation_count + (length_score / 50)

  example['difficulty'] = difficulty

  return example

dataset = dataset.map(compute_query_difficulty)

dataset = dataset.sort("difficulty")

train_dataset = dataset.select(range(100))

eval_dataset = dataset.select(range(100, 120))

compute_dtype = getattr(torch, "float16")

bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type='nf4',
        bnb_4bit_compute_dtype=compute_dtype,
        bnb_4bit_use_double_quant=False,
    )

model_name='google/flan-t5-large'

device_map = {"": 0}

original_model = AutoModelForSeq2SeqLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    trust_remote_code=True,
  )

tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True,
    padding_side="left",
    add_eos_token=True,
    add_bos_token=True,
    use_fast=False
  )

tokenizer.pad_token = tokenizer.eos_token

def gen(model, prompt, length):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    outputs = model.generate(**inputs, max_length=length, do_sample=True, top_p=0.95, temperature=0.8)

    return [tokenizer.decode(outputs[0], skip_special_tokens=True)]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# index = 2
# 
# prompt = dataset[index]['query']
# summary = dataset[index]['answers'][0]
# 
# formatted_prompt = f"Instruct: Refine this user search query.\n{prompt}"
# 
# res = gen(original_model,formatted_prompt,100,)
# 
# output = res[0]
# 
# dash_line = '-'.join('' for x in range(100))
# print(dash_line)
# print(f'INPUT PROMPT:\n{formatted_prompt}')
# print(dash_line)
# print(f'BASELINE HUMAN ANSWER:\n{summary}\n')
# print(dash_line)
# print(f'MODEL GENERATION - ZERO SHOT:\n{output}')

def create_prompt_formats(sample):
    INTRO_BLURB = "Below is an instruction that describes a task. Write a response that appropriately completes the request."

    prompt_templates = [
      "### Instruct: Refine this user search query:",
      "### Task: Improve the clarity of the following search query:",
      "### Instruction: Fix the grammar and phrasing of this e-commerce search input:",
      "### Command: Clean up this product search term:",
      "### Request: Make this user query more natural and readable:",
      "### Action: Rephrase this customer search for better understanding:",
    ]

    instruction = random.choice(prompt_templates) + f"\n{sample['query'].strip()}"
    target = sample['answers'][0].strip()

    return {
        "input": f"{INTRO_BLURB}\n\n{instruction}",
        "target": f"{target}"
    }

def get_max_length(model):
    conf = model.config
    max_length = None
    for length_setting in ["n_positions", "max_position_embeddings", "seq_length"]:
        max_length = getattr(model.config, length_setting, None)
        if max_length:
            print(f"Found max lenth: {max_length}")
            break
    if not max_length:
        max_length = 1024
        print(f"Using default max length: {max_length}")
    return max_length

def preprocess_batch(batch, tokenizer, max_length):
    """
    Tokenizing a batch
    """
    inputs = tokenizer(
        text_target=batch["input"],
        max_length=max_length,
        truncation=True,
        padding=False
    )

    targets = tokenizer(
        text_target=batch["target"],
        max_length=max_length,
        truncation=True,
        padding=False
    )

    return {
        "input_ids": inputs["input_ids"],
        "attention_mask": inputs["attention_mask"],
        "labels": targets["input_ids"]
    }

def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, dataset):
    """Format & tokenize it so it is ready for training
    :param tokenizer (AutoTokenizer): Model Tokenizer
    :param max_length (int): Maximum number of tokens to emit from tokenizer
    """

    print("Preprocessing dataset...")
    dataset = dataset.map(create_prompt_formats)#, batched=True)

    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)

    dataset = dataset.map(
        _preprocessing_function,
        batched=True,
    )

    dataset = dataset.remove_columns(['answers', 'query', 'difficulty', "input", "target"])

    dataset = dataset.filter(lambda sample: len(sample["input_ids"]) < max_length)

    return dataset

max_length = get_max_length(original_model)

train_dataset = preprocess_dataset(tokenizer, max_length, train_dataset)

eval_dataset = preprocess_dataset(tokenizer, max_length, eval_dataset)

original_model = prepare_model_for_kbit_training(original_model)

config = LoraConfig(
    r=32,
    lora_alpha=32,
    target_modules=["q", "v"],
    bias="none",
    lora_dropout=0.05,
    task_type="SEQ_2_SEQ_LM",
)

original_model.gradient_checkpointing_enable()

peft_model = get_peft_model(original_model, config)

def print_number_of_trainable_model_parameters(model):
    trainable_params = 0
    all_params = 0
    for _, param in model.named_parameters():
        all_params += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(f"Trainable params: {trainable_params}")
    print(f"All params: {all_params}")
    print(f"Trainable%: {100 * trainable_params / all_params:.2f}%")

print_number_of_trainable_model_parameters(peft_model)

output_dir = f'./peft-flan-t5-training-{str(int(time.time()))}'

peft_model.config.use_cache = False

peft_training_args = TrainingArguments(
    output_dir = output_dir,
    warmup_steps=100,
    per_device_train_batch_size=8,
    gradient_accumulation_steps=4,
    num_train_epochs=10,
    max_steps=5,
    learning_rate=1e-5,
    optim="paged_adamw_8bit",
    logging_steps=5,
    logging_dir="./logs",
    save_strategy="steps",
    save_steps=5,
    eval_strategy="steps",
    eval_steps=5,
    do_eval=True,
    gradient_checkpointing=True,
    report_to="none",
    overwrite_output_dir = 'True',
    group_by_length=True,
    load_best_model_at_end=True,
    dataloader_pin_memory=True,
    dataloader_num_workers=2,
    fp16=True
)

early_stopping = EarlyStoppingCallback(
    early_stopping_patience=5,
    early_stopping_threshold=0.0
    )

data_collator=DataCollatorForSeq2Seq(
    tokenizer,
    model=peft_model,
    padding="longest",
    pad_to_multiple_of=8
)

print("Sample processed batch:")
sample_batch = data_collator([train_dataset[0], train_dataset[1]])
print("Input shapes:", sample_batch["input_ids"].shape)
print("Label shapes:", sample_batch["labels"].shape)

bleu = evaluate.load("bleu")
rouge = evaluate.load("rouge")

def compute_metrics(eval_preds):
    preds, labels = eval_preds

    print(type(preds), type(preds[0]))
    print(preds[0])

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    bleu_result = bleu.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])

    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels)

    return {
        "bleu": bleu_result["bleu"],
        "rouge1": rouge_result["rouge1"],
        "rouge2": rouge_result["rouge2"],
        "rougeL": rouge_result["rougeL"]
    }

peft_trainer = transformers.Trainer(
    model=peft_model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    args=peft_training_args,
    data_collator=data_collator,
    callbacks=[early_stopping],
    # compute_metrics=compute_metrics
)

peft_trainer.train()

