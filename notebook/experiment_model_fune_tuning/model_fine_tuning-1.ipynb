{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score"
      ],
      "metadata": {
        "id": "0rXjy0EB0n-l"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    GenerationConfig,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from tqdm import tqdm\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from functools import partial\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import random\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "H8xCZFRq0u63"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['WANDB_DISABLED']=\"true\""
      ],
      "metadata": {
        "id": "skP0YRuN1Kc0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"microsoft/ms_marco\", \"v2.1\", split=\"train\")"
      ],
      "metadata": {
        "id": "5tkMint71UM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9421de52-32cd-4c29-fa82-69a86e73f3fa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.select(range(60))"
      ],
      "metadata": {
        "id": "C3hWg6NL1VYC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    return text.strip().lower()"
      ],
      "metadata": {
        "id": "aM9xZk9Z1gjE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(lambda example: {\n",
        "    'query': clean_text(example['query']),\n",
        "    'answers': [clean_text(ans) for ans in example['answers']]\n",
        "}, remove_columns=dataset.column_names)"
      ],
      "metadata": {
        "id": "gmZv_D0N1iRY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_query_difficulty(example):\n",
        "  query = example['query']\n",
        "\n",
        "  word_count = len(query.split())\n",
        "  punctuation_count = sum(1 for c in query if c in [',', '.', '?', '!', ':', ';'])\n",
        "\n",
        "  length_score = len(query)\n",
        "\n",
        "  difficulty = word_count + punctuation_count + (length_score / 50)\n",
        "\n",
        "  example['difficulty'] = difficulty\n",
        "\n",
        "  return example"
      ],
      "metadata": {
        "id": "duZKPdY_3hYp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(compute_query_difficulty)"
      ],
      "metadata": {
        "id": "jntzzLe33jkR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.sort(\"difficulty\")"
      ],
      "metadata": {
        "id": "Wyi085fu3o3w"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset.select(range(40))"
      ],
      "metadata": {
        "id": "BMwgfrcT3viS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = dataset.select(range(40, 60))"
      ],
      "metadata": {
        "id": "WYveYk913yZG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_dtype = getattr(torch, \"float16\")"
      ],
      "metadata": {
        "id": "g4uclYTh1jbE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type='nf4',\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_use_double_quant=False,\n",
        "    )"
      ],
      "metadata": {
        "id": "VRvMXOLf1luv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name='google/flan-t5-large'"
      ],
      "metadata": {
        "id": "i1vzh8dX1m_o"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device_map = {\"\": 0}"
      ],
      "metadata": {
        "id": "v5gX_yij1osU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True,\n",
        "  )"
      ],
      "metadata": {
        "id": "XYzSUYja1p8A"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    padding_side=\"left\",\n",
        "    add_eos_token=True,\n",
        "    add_bos_token=True,\n",
        "    use_fast=False\n",
        "  )"
      ],
      "metadata": {
        "id": "f2kmzoFM1sUU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c2f272b-123b-40f7-ac63-097e8a1b0104"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "hIlszuL01zdZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen(model, prompt, length):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)  # <-- send inputs to same device\n",
        "    outputs = model.generate(**inputs, max_length=length, do_sample=True, top_p=0.95, temperature=0.8)\n",
        "    return [tokenizer.decode(outputs[0], skip_special_tokens=True)]"
      ],
      "metadata": {
        "id": "yOxRH66k11Hi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%time\n",
        "\n",
        "index = 2\n",
        "\n",
        "prompt = dataset[index]['query']\n",
        "summary = dataset[index]['answers'][0]\n",
        "\n",
        "formatted_prompt = f\"Instruct: Refine this user search query.\\n{prompt}\"\n",
        "\n",
        "res = gen(original_model,formatted_prompt,100,)\n",
        "\n",
        "output = res[0]\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN ANSWER:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
      ],
      "metadata": {
        "id": "mEtNLQRd18Wk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24aadd98-2c38-4836-e9a2-d26a8897eac7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "INPUT PROMPT:\n",
            "Instruct: Refine this user search query.\n",
            "nyu tuition cost\n",
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE HUMAN ANSWER:\n",
            "$43,746 for the 2014-2015 academic year.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "MODEL GENERATION - ZERO SHOT:\n",
            "<pad> New York University tuition costs\n",
            "CPU times: user 954 ms, sys: 234 ms, total: 1.19 s\n",
            "Wall time: 4.35 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt_formats(sample):\n",
        "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
        "\n",
        "    prompt_templates = [\n",
        "      \"### Instruct: Refine this user search query:\",\n",
        "      \"### Task: Improve the clarity of the following search query:\",\n",
        "      \"### Instruction: Fix the grammar and phrasing of this e-commerce search input:\",\n",
        "      \"### Command: Clean up this product search term:\",\n",
        "      \"### Request: Make this user query more natural and readable:\",\n",
        "      \"### Action: Rephrase this customer search for better understanding:\",\n",
        "    ]\n",
        "\n",
        "    instruction = random.choice(prompt_templates) + f\"\\n{sample['query'].strip()}\"\n",
        "    target = sample['answers'][0].strip()\n",
        "\n",
        "    return {\n",
        "        \"input\": f\"{INTRO_BLURB}\\n\\n{instruction}\",\n",
        "        \"target\": f\"{target}\"\n",
        "    }"
      ],
      "metadata": {
        "id": "HkqJTDN119w8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_length(model):\n",
        "    conf = model.config\n",
        "    max_length = None\n",
        "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
        "        max_length = getattr(model.config, length_setting, None)\n",
        "        if max_length:\n",
        "            print(f\"Found max lenth: {max_length}\")\n",
        "            break\n",
        "    if not max_length:\n",
        "        max_length = 1024\n",
        "        print(f\"Using default max length: {max_length}\")\n",
        "    return max_length"
      ],
      "metadata": {
        "id": "UAPbPNtt2Cp6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_batch(batch, tokenizer, max_length):\n",
        "    model_inputs = tokenizer(\n",
        "        batch[\"input\"],\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\",\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "    labels = tokenizer(\n",
        "        batch[\"target\"],\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "tUMc0NMq2EfT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, dataset):\n",
        "    \"\"\"Format & tokenize it so it is ready for training\n",
        "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
        "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Preprocessing dataset...\")\n",
        "    dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
        "\n",
        "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
        "\n",
        "    dataset = dataset.map(\n",
        "        _preprocessing_function,\n",
        "        batched=True,\n",
        "    )\n",
        "\n",
        "    dataset = dataset.remove_columns(['answers', 'query', 'difficulty', \"input\", \"target\"])\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "YEDnrI9m2Fzw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = get_max_length(original_model)"
      ],
      "metadata": {
        "id": "jwTuaZuZ2JRj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c36559c-1beb-4193-b2bb-3c01a87be848"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found max lenth: 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = preprocess_dataset(tokenizer, max_length, train_dataset)"
      ],
      "metadata": {
        "id": "moZuH1Wc2KwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5db05555-f000-45f7-be87-0ed87b1476fe"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = preprocess_dataset(tokenizer, max_length, eval_dataset)"
      ],
      "metadata": {
        "id": "22kFbeeq2Ocz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b39b214-20ad-4a0d-c4c8-3994dff5a27d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_model = prepare_model_for_kbit_training(original_model)"
      ],
      "metadata": {
        "id": "YPXDazwo2Oyt"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"SEQ_2_SEQ_LM\",\n",
        ")"
      ],
      "metadata": {
        "id": "gXz1SvEn2Q8F"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_model.gradient_checkpointing_enable()"
      ],
      "metadata": {
        "id": "vXavsY-b2SWr"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = get_peft_model(original_model, config)"
      ],
      "metadata": {
        "id": "wkxzvjcc2UGr"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_params = 0\n",
        "    all_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(f\"Trainable params: {trainable_params}\")\n",
        "    print(f\"All params: {all_params}\")\n",
        "    print(f\"Trainable%: {100 * trainable_params / all_params:.2f}%\")"
      ],
      "metadata": {
        "id": "af5MK6uL2Vma"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_number_of_trainable_model_parameters(peft_model)"
      ],
      "metadata": {
        "id": "c0WHQ2pY2XJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af79f21b-f474-4b24-87eb-16466cdeb6da"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable params: 9437184\n",
            "All params: 503180288\n",
            "Trainable%: 1.88%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = f'./peft-flan-t5-training-{str(int(time.time()))}'"
      ],
      "metadata": {
        "id": "YsPI6kjj2Yve"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model.config.use_cache = False"
      ],
      "metadata": {
        "id": "6iGH7sgv2aZm"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_training_args = TrainingArguments(\n",
        "    output_dir = output_dir,\n",
        "    warmup_steps=1,\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=2,\n",
        "    max_steps=3,\n",
        "    learning_rate=2e-4,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    logging_steps=25,\n",
        "    logging_dir=\"./logs\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=25,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=25,\n",
        "    do_eval=True,\n",
        "    gradient_checkpointing=True,\n",
        "    report_to=\"none\",\n",
        "    overwrite_output_dir = 'True',\n",
        "    group_by_length=True,\n",
        "    load_best_model_at_end=True\n",
        ")"
      ],
      "metadata": {
        "id": "y1EbKBhE2bmH"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=original_model,\n",
        ")"
      ],
      "metadata": {
        "id": "TGK5uFdh5e-5"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")"
      ],
      "metadata": {
        "id": "JXk3_Ako_U3b"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    bleu_result = bleu.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n",
        "\n",
        "    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    return {\n",
        "        \"bleu\": bleu_result[\"bleu\"],\n",
        "        \"rouge1\": rouge_result[\"rouge1\"],\n",
        "        \"rouge2\": rouge_result[\"rouge2\"],\n",
        "        \"rougeL\": rouge_result[\"rougeL\"]\n",
        "    }"
      ],
      "metadata": {
        "id": "SF20lpjc_Oyl"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStoppingCallback(\n",
        "    early_stopping_patience=1,\n",
        "    early_stopping_threshold=0.0\n",
        "    )"
      ],
      "metadata": {
        "id": "Cd0n2uOTAP4Q"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    args=peft_training_args,\n",
        "    data_collator = data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[early_stopping],\n",
        ")"
      ],
      "metadata": {
        "id": "N-mODgdT2cqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "063a2810-b10c-4e42-a898-24bee038c596"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peft_trainer.train()"
      ],
      "metadata": {
        "id": "QG7Zkqi02ebf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "13c5c5dc-aa25-419f-ffb4-cc87de94efb8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:30, Epoch 1/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3, training_loss=1.6287754376729329, metrics={'train_runtime': 53.9934, 'train_samples_per_second': 1.778, 'train_steps_per_second': 0.056, 'total_flos': 168030646566912.0, 'train_loss': 1.6287754376729329, 'epoch': 1.8})"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model.save_pretrained(\"model-fine-tune\")"
      ],
      "metadata": {
        "id": "SMFXGIa2BpnK"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"token-fine-tune\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3_5d6ZCCDvv",
        "outputId": "bc52a5b4-c2af-4b30-b9ea-f9f9feb9b793"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('token-fine-tune/tokenizer_config.json',\n",
              " 'token-fine-tune/special_tokens_map.json',\n",
              " 'token-fine-tune/spiece.model',\n",
              " 'token-fine-tune/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = peft_model.from_pretrained(original_model, \"model-fine-tune\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb-f2dmqCNX1",
        "outputId": "035b862f-e013-4ecc-a372-a12cd1334e6c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y4c-wiQDcyw",
        "outputId": "41d34069-fcce-42bc-c8f4-463bf4843ed7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForSeq2SeqLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): T5ForConditionalGeneration(\n",
              "      (shared): Embedding(32128, 1024)\n",
              "      (encoder): T5Stack(\n",
              "        (embed_tokens): Embedding(32128, 1024)\n",
              "        (block): ModuleList(\n",
              "          (0): T5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): T5LayerSelfAttention(\n",
              "                (SelfAttention): T5Attention(\n",
              "                  (q): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (k): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
              "                  (v): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (o): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
              "                  (relative_attention_bias): Embedding(32, 16)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): T5LayerFF(\n",
              "                (DenseReluDense): T5DenseGatedActDense(\n",
              "                  (wi_0): Linear4bit(in_features=1024, out_features=2816, bias=False)\n",
              "                  (wi_1): Linear4bit(in_features=1024, out_features=2816, bias=False)\n",
              "                  (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (1-23): 23 x T5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): T5LayerSelfAttention(\n",
              "                (SelfAttention): T5Attention(\n",
              "                  (q): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (k): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
              "                  (v): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (o): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): T5LayerFF(\n",
              "                (DenseReluDense): T5DenseGatedActDense(\n",
              "                  (wi_0): Linear4bit(in_features=1024, out_features=2816, bias=False)\n",
              "                  (wi_1): Linear4bit(in_features=1024, out_features=2816, bias=False)\n",
              "                  (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (final_layer_norm): T5LayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (decoder): T5Stack(\n",
              "        (embed_tokens): Embedding(32128, 1024)\n",
              "        (block): ModuleList(\n",
              "          (0): T5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): T5LayerSelfAttention(\n",
              "                (SelfAttention): T5Attention(\n",
              "                  (q): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (k): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
              "                  (v): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (o): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
              "                  (relative_attention_bias): Embedding(32, 16)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): T5LayerCrossAttention(\n",
              "                (EncDecAttention): T5Attention(\n",
              "                  (q): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (k): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
              "                  (v): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (o): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (2): T5LayerFF(\n",
              "                (DenseReluDense): T5DenseGatedActDense(\n",
              "                  (wi_0): Linear4bit(in_features=1024, out_features=2816, bias=False)\n",
              "                  (wi_1): Linear4bit(in_features=1024, out_features=2816, bias=False)\n",
              "                  (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (1-23): 23 x T5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): T5LayerSelfAttention(\n",
              "                (SelfAttention): T5Attention(\n",
              "                  (q): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (k): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
              "                  (v): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (o): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): T5LayerCrossAttention(\n",
              "                (EncDecAttention): T5Attention(\n",
              "                  (q): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (k): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
              "                  (v): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (o): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (2): T5LayerFF(\n",
              "                (DenseReluDense): T5DenseGatedActDense(\n",
              "                  (wi_0): Linear4bit(in_features=1024, out_features=2816, bias=False)\n",
              "                  (wi_1): Linear4bit(in_features=1024, out_features=2816, bias=False)\n",
              "                  (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): T5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (final_layer_norm): T5LayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"token-fine-tune\")"
      ],
      "metadata": {
        "id": "27c5WDiJCOVZ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Instruct: Refine this user search query.\\nred shoes men casual\"\n",
        "output = gen(model, prompt, 100)\n",
        "print(\"Refined Query:\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eP5djbooC-iM",
        "outputId": "2f98ff0a-a287-435b-d37b-d73776e5dfd8"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Refined Query: ['red shoes for men casual']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D4VnUTmkDCkV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}