# -*- coding: utf-8 -*-
"""model_fine_tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qCuxV6eQeNFVO-nj2x0ISBdvW9ukU0af
"""

!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score

from datasets import load_dataset
import transformers
from transformers import (
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    HfArgumentParser,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    GenerationConfig
)
from tqdm import tqdm
from trl import SFTTrainer
import torch
import time
import pandas as pd
import numpy as np
import os
from functools import partial
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

os.environ['WANDB_DISABLED']="true"

dataset = load_dataset("microsoft/ms_marco", "v2.1", split="train")

subset = dataset.select(range(50))

len(subset)

def clean_text(text):
    return text.strip().lower()

subset = subset.map(lambda example: {
    'query': clean_text(example['query']),
    'answers': [clean_text(ans) for ans in example['answers']]
})

subset

len(subset)

compute_dtype = getattr(torch, "float16")

bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type='nf4',
        bnb_4bit_compute_dtype=compute_dtype,
        bnb_4bit_use_double_quant=False,
    )

model_name='google/flan-t5-large'

device_map = {"": 0}

original_model = AutoModelForSeq2SeqLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    trust_remote_code=True,
  )

tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True,
    padding_side="left",
    add_eos_token=True,
    add_bos_token=True,
    use_fast=False
  )

tokenizer.pad_token = tokenizer.eos_token

def gen(model, prompt, length):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    inputs = tokenizer(prompt, return_tensors="pt").to(device)  # <-- send inputs to same device
    outputs = model.generate(**inputs, max_length=length, do_sample=True, top_p=0.95, temperature=0.8)
    return [tokenizer.decode(outputs[0], skip_special_tokens=True)]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# index = 2
# 
# prompt = dataset[index]['query']
# summary = dataset[index]['answers'][0]
# 
# formatted_prompt = f"Instruct: Refine this user search query.\n{prompt}"
# 
# res = gen(original_model,formatted_prompt,100,)
# 
# output = res[0]
# 
# dash_line = '-'.join('' for x in range(100))
# print(dash_line)
# print(f'INPUT PROMPT:\n{formatted_prompt}')
# print(dash_line)
# print(f'BASELINE HUMAN ANSWER:\n{summary}\n')
# print(dash_line)
# print(f'MODEL GENERATION - ZERO SHOT:\n{output}')

def create_prompt_formats(sample):
    INTRO_BLURB = "Below is an instruction that describes a task. Write a response that appropriately completes the request."
    INSTRUCTION_KEY = "### Instruct: Refine this user search query:"
    RESPONSE_KEY = "### Output:"
    END_KEY = "### End"

    blurb = f"\n{INTRO_BLURB}"
    instruction = f"{INSTRUCTION_KEY}\n{sample['query'].strip()}"

    # Use first available answer or a default fallback
    answer = sample['answers'][0].strip() if sample.get('answers') and len(sample['answers']) > 0 else "No Answer Present."

    response = f"{RESPONSE_KEY}\n{answer}"
    end = f"{END_KEY}"

    # Compose final formatted string
    parts = [blurb, instruction, response, end]
    formatted_prompt = "\n\n".join(parts)

    sample["text"] = formatted_prompt
    return sample

def get_max_length(model):
    conf = model.config
    max_length = None
    for length_setting in ["n_positions", "max_position_embeddings", "seq_length"]:
        max_length = getattr(model.config, length_setting, None)
        if max_length:
            print(f"Found max lenth: {max_length}")
            break
    if not max_length:
        max_length = 1024
        print(f"Using default max length: {max_length}")
    return max_length

def preprocess_batch(batch, tokenizer, max_length):
    """
    Tokenizing a batch
    """
    return tokenizer(
        batch["text"],
        max_length=max_length,
        truncation=True,
    )

def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):
    """Format & tokenize it so it is ready for training
    :param tokenizer (AutoTokenizer): Model Tokenizer
    :param max_length (int): Maximum number of tokens to emit from tokenizer
    """

    print("Preprocessing dataset...")
    dataset = dataset.map(create_prompt_formats)#, batched=True)

    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)
    dataset = dataset.map(
        _preprocessing_function,
        batched=True,
        remove_columns=['passages', 'query_id', 'query_type', 'wellFormedAnswers'],
    )

    dataset = dataset.filter(lambda sample: len(sample["input_ids"]) < max_length)

    dataset = dataset.shuffle(seed=seed)

    return dataset

seed = 42

max_length = get_max_length(original_model)
print(max_length)

train_dataset = preprocess_dataset(tokenizer, max_length,seed, subset)
eval_dataset = preprocess_dataset(tokenizer, max_length,seed, subset)

train_dataset

original_model = prepare_model_for_kbit_training(original_model)

config = LoraConfig(
    r=32,
    lora_alpha=32,
    target_modules=["q", "v"],
    bias="none",
    lora_dropout=0.05,
    task_type="SEQ_2_SEQ_LM",
)

original_model.gradient_checkpointing_enable()

peft_model = get_peft_model(original_model, config)

def print_number_of_trainable_model_parameters(model):
    trainable_params = 0
    all_params = 0
    for _, param in model.named_parameters():
        all_params += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(f"Trainable params: {trainable_params}")
    print(f"All params: {all_params}")
    print(f"Trainable%: {100 * trainable_params / all_params:.2f}%")

print_number_of_trainable_model_parameters(peft_model)

output_dir = f'./peft-flan-t5-training-{str(int(time.time()))}'

peft_model.config.use_cache = False

peft_training_args = TrainingArguments(
    output_dir = output_dir,
    warmup_steps=1,
    per_device_train_batch_size=8,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    max_steps=5,
    learning_rate=2e-4,
    optim="paged_adamw_8bit",
    logging_steps=25,
    logging_dir="./logs",
    save_strategy="steps",
    save_steps=25,
    eval_strategy="steps",
    eval_steps=25,
    do_eval=True,
    gradient_checkpointing=True,
    report_to="none",
    overwrite_output_dir = 'True',
    group_by_length=True,
)

peft_trainer = transformers.Trainer(
    model=peft_model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    args=peft_training_args,
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
)

peft_trainer.train()

