# -*- coding: utf-8 -*-
"""model_fine_tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qCuxV6eQeNFVO-nj2x0ISBdvW9ukU0af

# Install Required Packages
"""

!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score

"""# Import Packages"""

from datasets import load_dataset
import transformers
from transformers import (
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    HfArgumentParser,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    GenerationConfig,
    DataCollatorForSeq2Seq,
    EarlyStoppingCallback
)
from tqdm import tqdm
from trl import SFTTrainer
import torch
import time
import pandas as pd
import numpy as np
import os
from functools import partial
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import random
import nltk
import evaluate

"""# Define Environment Variables"""

os.environ['WANDB_DISABLED']="true"

"""# Load Dataset"""

dataset = load_dataset("microsoft/ms_marco", "v2.1", split="train")

dataset = dataset.select(range(60))

"""# Preprocess Queries String"""

def clean_text(text):
    return text.strip().lower()

dataset = dataset.map(lambda example: {
    'query': clean_text(example['query']),
    'answers': [clean_text(ans) for ans in example['answers']]
}, remove_columns=dataset.column_names)

"""# Ranking With Query Complexity"""

def compute_query_difficulty(example):
  query = example['query']

  word_count = len(query.split())
  punctuation_count = sum(1 for c in query if c in [',', '.', '?', '!', ':', ';'])

  length_score = len(query)

  difficulty = word_count + punctuation_count + (length_score / 50)

  example['difficulty'] = difficulty

  return example

dataset = dataset.map(compute_query_difficulty)

dataset = dataset.sort("difficulty")

"""# Spliting Train And Eval Dataset"""

train_dataset = dataset.select(range(40))

eval_dataset = dataset.select(range(40, 60))

"""# Configure Quantization"""

bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type='nf4',
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=False,
    )

"""# Download Model From Hugging Face"""

model_name='google/flan-t5-base'

original_model = AutoModelForSeq2SeqLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    trust_remote_code=True,
  )

"""# Define Embedding"""

tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True,
    padding_side="left",
    add_eos_token=True,
    add_bos_token=True,
    use_fast=False
  )

tokenizer.model_max_length = 128
tokenizer.pad_token = tokenizer.eos_token

"""# Model Output Compare Function"""

def generate_and_compare(
    model, tokenizer, prompt, summary, length=100,
    prefix="Instruct: Refine this user search query.",
    temperature=0.8, top_p=0.95
):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    formatted_prompt = f"{prefix}\n{prompt}"

    inputs = tokenizer(formatted_prompt, return_tensors="pt").to(device)
    outputs = model.generate(
        **inputs, max_length=length,
        do_sample=True, top_p=top_p, temperature=temperature
    )

    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)

    dash_line = '-' * 100
    print(dash_line)
    print(f'INPUT PROMPT:\n{formatted_prompt}')
    print(dash_line)
    print(f'BASELINE HUMAN ANSWER:\n{summary}\n')
    print(dash_line)
    print(f'MODEL GENERATION - ZERO SHOT:\n{decoded_output}')
    print(dash_line)

    return decoded_output

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# prompt = dataset[2]['query']
# summary = dataset[2]['answers'][0]
# 
# test_output = generate_and_compare(
#     model=original_model,
#     tokenizer=tokenizer,
#     prompt=prompt,
#     summary=summary,
# )
# 
# test_output

"""# Prompt Engineering"""

def create_prompt_formats(sample):
    INTRO_BLURB = "Below is an instruction that describes a task. Write a response that appropriately completes the request."

    prompt_templates = [
      "### Instruct: Refine this user search query:",
      "### Task: Improve the clarity of the following search query:",
      "### Instruction: Fix the grammar and phrasing of this e-commerce search input:",
      "### Command: Clean up this product search term:",
      "### Request: Make this user query more natural and readable:",
      "### Action: Rephrase this customer search for better understanding:",
    ]

    instruction = random.choice(prompt_templates) + f"\n{sample['query'].strip()}"
    target = sample['answers'][0].strip()

    return {
        "input": f"{INTRO_BLURB}\n\n{instruction}",
        "target": f"{target}"
    }

"""# Get Model Max Length"""

def get_max_length(model):
    conf = model.config
    max_length = None
    for length_setting in ["n_positions", "max_position_embeddings", "seq_length"]:
        max_length = getattr(model.config, length_setting, None)
        if max_length:
            print(f"Found max lenth: {max_length}")
            break
    if not max_length:
        max_length = 1024
        print(f"Using default max length: {max_length}")
    return max_length

"""# Process Text To Embedding"""

def preprocess_batch(batch, tokenizer, max_length):
    model_inputs = tokenizer(
        batch["input"],
        max_length=max_length,
        truncation=True,
        padding="max_length",
        return_tensors="pt",
        return_attention_mask=True
    )

    labels = tokenizer(
        batch["target"],
        max_length=max_length,
        truncation=True,
        padding="max_length",
        return_tensors="pt",
    )

    model_inputs["labels"] = labels["input_ids"]

    return model_inputs

max_length = get_max_length(original_model)

"""# Process Dataset For Model"""

def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, dataset):
    """Format & tokenize it so it is ready for training
    :param tokenizer (AutoTokenizer): Model Tokenizer
    :param max_length (int): Maximum number of tokens to emit from tokenizer
    """

    print("Preprocessing dataset...")
    dataset = dataset.map(create_prompt_formats)#, batched=True)

    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)

    dataset = dataset.map(
        _preprocessing_function,
        batched=True,
    )

    dataset = dataset.remove_columns(['answers', 'query', 'difficulty', "input", "target"])

    return dataset

train_dataset = preprocess_dataset(tokenizer, 128, train_dataset)

eval_dataset = preprocess_dataset(tokenizer, 128, eval_dataset)

"""# Config Low Rank Adaptation"""

original_model = prepare_model_for_kbit_training(original_model)

config = LoraConfig(
    r=4,
    lora_alpha=8,
    target_modules=["q", "v"],
    bias="none",
    lora_dropout=0.0,
    task_type="SEQ_2_SEQ_LM",
)

config.inference_mode = False
original_model.gradient_checkpointing_enable()

peft_model = get_peft_model(original_model, config)

peft_model.config.use_cache = False
peft_model.config.pretraining_tp = 1
peft_model.generation_config.pad_token_id = tokenizer.pad_token_id
peft_model.generation_config.eos_token_id = tokenizer.eos_token_id
peft_model.config.max_length = 128

"""# Compare QLoRA Model Parameter"""

def print_number_of_trainable_model_parameters(model):
    trainable_params = 0
    all_params = 0
    for _, param in model.named_parameters():
        all_params += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(f"Trainable params: {trainable_params}")
    print(f"All params: {all_params}")
    print(f"Trainable%: {100 * trainable_params / all_params:.2f}%")

print_number_of_trainable_model_parameters(peft_model)

"""# Define Model Training Arguments"""

output_dir = f'./peft-flan-t5-training-{str(int(time.time()))}'

peft_training_args = TrainingArguments(
    output_dir = output_dir,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=16,
    num_train_epochs=1,
    max_steps=50,
    learning_rate=1e-4,
    fp16=False,
    optim="paged_adamw_8bit",
    logging_steps=5,
    eval_steps=5,
    save_total_limit=1,
    gradient_checkpointing=True,
    dataloader_num_workers=2,
    remove_unused_columns=True,
    do_eval=True,
    eval_strategy="steps",
    load_best_model_at_end=True,
    disable_tqdm=False
)

"""# Model Data Collection"""

data_collator = DataCollatorForSeq2Seq(
    tokenizer,
    model=original_model,
)

"""# Making Traing Callbacks"""

early_stopping = EarlyStoppingCallback(
    early_stopping_patience=3,
    early_stopping_threshold=0.1,
    )

"""# Model Evaluation Metrics"""

def compute_metrics(eval_preds):
    rouge = evaluate.load('rouge')
    bleu = evaluate.load('bleu')
    meteor = evaluate.load('meteor')

    preds = eval_preds.predictions
    labels = eval_preds.label_ids

    if isinstance(preds, tuple):
      preds = preds[0]

    pred_ids = np.argmax(preds, axis=-1)

    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)

    decoded_preds = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    rouge_results = rouge.compute(
        predictions=decoded_preds,
        references=decoded_labels,
        use_stemmer=True
    )
    rouge_results = {k: round(v * 100, 4) for k, v in rouge_results.items()}

    bleu_results = bleu.compute(
        predictions=decoded_preds,
        references=[[ref] for ref in decoded_labels]
    )
    bleu_results = {'bleu': round(bleu_results['bleu'] * 100, 4)}

    meteor_results = meteor.compute(
        predictions=decoded_preds,
        references=decoded_labels
    )
    meteor_results = {'meteor': round(meteor_results['meteor'] * 100, 4)}

    metrics = {
        **rouge_results,
        **bleu_results,
        **meteor_results
    }

    return metrics

"""# Define Model Trainer"""

peft_trainer = Trainer(
    model=peft_model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    args=peft_training_args,
    data_collator = data_collator,
    compute_metrics=compute_metrics,
    callbacks=[early_stopping],
)

"""# Fitting The Model"""

peft_trainer.train()

"""# Save Model And Embedding"""

peft_model.save_pretrained("model-fine-tune")

tokenizer.save_pretrained("token-fine-tune")

"""# Download And Evaluate The Model"""

model = peft_model.from_pretrained(original_model, "model-fine-tune")

model.eval()

tokenizer = AutoTokenizer.from_pretrained("token-fine-tune")

prompt = "new dress for new girl"

test_output = generate_and_compare(
    model=original_model,
    tokenizer=tokenizer,
    prompt=prompt,
    summary="",
)

test_output

